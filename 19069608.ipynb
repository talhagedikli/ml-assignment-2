{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML-Assignment 2\n",
    "Muhammed Talha Gedikli - 19069608"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC #note that this is non-linear SVM and its default is RBF\n",
    "from sklearn.datasets import make_regression #function used for generating synthetic datasets\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Import data\n",
    "train_data = pd.read_excel(\"./asg1_train_test_data/asg1_traindata.xlsx\")\n",
    "test_data = pd.read_excel(\"./asg1_train_test_data/asg1_testdata_without_prices.xlsx\")\n",
    "\n",
    "test_data_without_prices = pd.read_excel(\"./asg1_train_test_data/asg1_testdata_without_prices.xlsx\")\n",
    "\n",
    "features_X = [\"Year\", \"Type\", \"Shift\", \"km\", \"Power\", \"Engine\", \"Seller\"]\n",
    "features_y = [\"Price\"]\n",
    "\n",
    "\n",
    "# Split data into features - prices\n",
    "X_data = train_data[features_X]\n",
    "y_data = train_data[features_y]\n",
    "\n",
    "X_data_without_prices = test_data_without_prices\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "# default is 75% / 25% train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state=735)\n",
    "\n",
    "# X_test = test_data[features_X]\n",
    "# y_test = test_data[features_y] Y values of test is not given\n",
    "# y_test = pd.read_csv(\"./Result.txt\", sep=\" \", header=None) S\n",
    "\n",
    "# default 'n' value for knn or cv\n",
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n# Deal with categorical data\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n# Find the columns to deal with\\nobject_cols = [col for col in X_data.columns if X_data[col].dtype == \"object\"]\\n\\n# Apply one-hot encoder to each column with categorical data\\nOH_encoder = OneHotEncoder(handle_unknown=\\'ignore\\', sparse=False)\\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\\nOH_cols_test = pd.DataFrame(OH_encoder.fit_transform(X_test[object_cols]))\\n\\n# Give them column names (for example \"Shift_Auto, Shift_Manual\") instead of just integer column names (like \"0, 1, 2\")\\nOH_cols_train.columns = OH_encoder.get_feature_names_out(object_cols)\\nOH_cols_test.columns = OH_encoder.get_feature_names_out(object_cols)\\n\\n# One-hot encoding removed index; put it back\\nOH_cols_train.index = X_train.index\\nOH_cols_test.index = X_test.index\\n\\n# Remove categorical columns (will replace with one-hot encoding)\\nnum_X_train = X_train.drop(object_cols, axis=1)\\nnum_X_test = X_test.drop(object_cols, axis=1)\\n\\n# Add one-hot encoded columns to numerical features\\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\\nOH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "# Deal with categorical data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Find the columns to deal with\n",
    "object_cols = [col for col in X_data.columns if X_data[col].dtype == \"object\"]\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_test = pd.DataFrame(OH_encoder.fit_transform(X_test[object_cols]))\n",
    "\n",
    "# Give them column names (for example \"Shift_Auto, Shift_Manual\") instead of just integer column names (like \"0, 1, 2\")\n",
    "OH_cols_train.columns = OH_encoder.get_feature_names_out(object_cols)\n",
    "OH_cols_test.columns = OH_encoder.get_feature_names_out(object_cols)\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_test.index = X_test.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_test = X_test.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle With Categorical Data and Do Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with categorical data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Find the columns to deal with\n",
    "object_cols = [col for col in X_data.columns if X_data[col].dtype == \"object\"]\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols = pd.DataFrame(OH_encoder.fit_transform(X_data[object_cols]))\n",
    "OH_X_without_prices_cols = pd.DataFrame(OH_encoder.fit_transform(X_data_without_prices[object_cols]))\n",
    "\n",
    "\n",
    "# Give them column names (for example \"Shift_Auto, Shift_Manual\") instead of just integer column names (like \"0, 1, 2\")\n",
    "OH_cols.columns = OH_encoder.get_feature_names_out(object_cols)\n",
    "OH_X_without_prices_cols.columns = OH_encoder.get_feature_names_out(object_cols)\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols.index = X_data.index\n",
    "OH_X_without_prices_cols.index = X_data_without_prices.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_data.drop(object_cols, axis=1)\n",
    "num_X_without_prices = X_data_without_prices.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_data = pd.concat([num_X_train, OH_cols], axis=1)\n",
    "OH_X_data_without_prices = pd.concat([num_X_without_prices, OH_X_without_prices_cols], axis=1)\n",
    "\n",
    "# Split data into train and test sets\n",
    "# default is 75% / 25% train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(OH_X_data, y_data, random_state=735)\n",
    "\n",
    "# Feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "##X_train, X_test, y_train, y_test = train_test_split(X_house, y_house)\n",
    "\n",
    "# Scale data\n",
    "X_train_scaled = scaler.fit_transform(X_train) # we use X_train to adjust/fit the scaler\n",
    "X_test_scaled  = scaler.transform(X_test) #use the same fit found above to transform X_test# Scale data\n",
    "\n",
    "OH_X_data_scaled = scaler.fit_transform(OH_X_data) # we use X_train to adjust/fit the scaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineer Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 0.862\n",
      "R-squared score (test): 0.890\n",
      "\n",
      "CV Score (test): 0.864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Prepare Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(OH_X_data, y_data, random_state=5)\n",
    "\n",
    "# Define the model\n",
    "linr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "kf5 = KFold(n_splits=N, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get and save the r2 values of the model\n",
    "linr_scores = {\n",
    "     \"r2_train\": linr.score(X_train, y_train),\n",
    "     \"r2_test\": linr.score(X_test, y_test),\n",
    "     \"r2_cv\": cross_val_score(linr, X_train, y_train, cv=N, scoring=\"r2\")\n",
    "}\n",
    "\n",
    "# Print the scores\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linr_scores['r2_train']))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linr_scores['r2_test']))\n",
    "print('CV Score (test): {:.3f}\\n'\n",
    "     .format(linr_scores['r2_cv'].mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression #function used for generating synthetic datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# # Make friedman\n",
    "# X_F1, y_F1 = make_friedman1(n_samples = 100,\n",
    "#                            n_features = 7, random_state=0)\n",
    "\n",
    "# # Prepare data for training and test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_F1, y_F1, random_state = 0)\n",
    "\n",
    "# Set polynomial features\n",
    "poly_degree = 1\n",
    "poly = PolynomialFeatures(degree=poly_degree)\n",
    "X_F1_poly = poly.fit_transform(OH_X_data)\n",
    "\n",
    "# Prepare data to make polynomial regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_data, random_state = 1)\n",
    "polyr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "kf5 = KFold(n_splits=N, shuffle=True)\n",
    "\n",
    "polyr_degree = 0\n",
    "polyr_degree = 0\n",
    "kf5 = KFold(n_splits=N, shuffle=True)\n",
    "\n",
    "for this_degree in [1, 2, 3, 4, 5]:\n",
    "     poly = PolynomialFeatures(degree=this_degree)\n",
    "     X_F1_poly = poly.fit_transform(OH_X_data)\n",
    "     # Prepare data to make polynomial regression\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_data, random_state = 1)\n",
    "     polyr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "     this_score = polyr.score(X_test, y_test)\n",
    "     if (this_score > polyr_degree):\n",
    "          polyr_degree = this_score\n",
    "          polyr_degree = this_degree\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree=polyr_degree)\n",
    "X_F1_poly = poly.fit_transform(OH_X_data)\n",
    "# Prepare data to make polynomial regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_data, random_state = 1)\n",
    "polyr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Get and save the r2 value of the model\n",
    "polyr_scores = {\n",
    "     'optimal_degree': poly_degree,\n",
    "     'r2_train': polyr.score(X_train, y_train),\n",
    "     'r2_test': polyr.score(X_test, y_test),\n",
    "     'r2_cv': cross_val_score(polyr, X_train, y_train, cv=kf5, scoring=\"r2\")\n",
    "}\n",
    "\n",
    "# Print r2 value\n",
    "# print('R-squared score (training): {:.3f}'\n",
    "#      .format(polr_scores['r2_train']))\n",
    "# print('R-squared score (test): {:.3f}\\n'\n",
    "#      .format(polr_scores['r2_test']))\n",
    "# print('R-squared score (test): {:.3f}\\n'\n",
    "#      .format(linr_scores['r2_cv'].mean()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal_alpha: 1.37\n",
      "r2_train: 0.862063615440508\n",
      "r2_test: 0.8899703103623747\n",
      "r2_cv: 0.8620229418077487\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Prepare data \n",
    "X_train, X_test, y_train, y_test = train_test_split(OH_X_data, y_data, random_state=735)\n",
    "\n",
    "\n",
    "# for this_alpha in [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40]:\n",
    "ridgr_alpha = 0\n",
    "ridgr_cv = 0\n",
    "kf5 = KFold(n_splits=N, shuffle=True)\n",
    "for this_a in [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.35, 1.37, 1.43, 1.45]:\n",
    "     ridgr = Ridge(alpha=this_a, random_state=1).fit(X_train, y_train)\n",
    "     this_cv = cross_val_score(ridgr, X_train, y_train, cv=N, scoring='r2').mean()\n",
    "     if (this_cv > ridgr_cv):\n",
    "          ridgr_cv = this_cv\n",
    "          ridgr_alpha = this_a\n",
    "\n",
    "# Define final model with best alpha\n",
    "ridgr = Ridge(alpha=ridgr_alpha, random_state=1).fit(X_train, y_train)\n",
    "\n",
    "# Get and save r2 values of the model\n",
    "ridgr_scores = {\n",
    "     'optimal_alpha': ridgr_alpha,\n",
    "     'r2_train': ridgr.score(X_train, y_train),\n",
    "     'r2_test': ridgr.score(X_test, y_test),\n",
    "     'r2_cv': cross_val_score(ridgr, X_train, y_train, cv=kf5, scoring=\"r2\").mean()\n",
    "}\n",
    "\n",
    "for k, v in ridgr_scores.items():\n",
    "     print(k + \": \" + str(v))\n",
    "# Print r2 values of the model\n",
    "# print('R-squared score (training): {:.3f}'\n",
    "#      .format(ridgr_scores['r2_train']))\n",
    "# print('R-squared score (test): {:.3f}\\n'\n",
    "#      .format(ridgr_scores['r2_test']))\n",
    "# print('R-squared score (test): {:.3f}\\n'\n",
    "#      .format(ridgr_scores['r2_cv'].mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talha\\AppData\\Local\\Temp\\ipykernel_11716\\3812617348.py:11: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lassr = Lasso(alpha=this_a, random_state=1).fit(X_train, y_train)\n",
      "c:\\Users\\talha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\talha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.363e+10, tolerance: 3.549e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.8696459614651998\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(OH_X_data, y_data, random_state=1)\n",
    "\n",
    "lassr_alpha = 0\n",
    "lassr_cv = 0\n",
    "kf5 = KFold(n_splits=N, shuffle=True)\n",
    "for this_a in [0, 0.0001, 0.001, 0.01, 0.1]:\n",
    "     lassr = Lasso(alpha=this_a, random_state=1).fit(X_train, y_train)\n",
    "     this_cv = cross_val_score(ridgr, X_train, y_train, cv=5, scoring='r2').mean()\n",
    "     if (this_cv > lassr_cv):\n",
    "          lassr_cv = this_cv\n",
    "          lassr_alpha = this_a\n",
    "\n",
    "\n",
    "\n",
    "# Get and save r2 scores of the model\n",
    "lassr_scores = {\n",
    "     'optimal_alpha': lassr_alpha,\n",
    "     'r2_train': lassr.score(X_train, y_train),\n",
    "     'r2_test': lassr.score(X_test, y_test),\n",
    "     'r2_cv': cross_val_score(lassr, X_train, y_train, cv=kf5, scoring=\"r2\")\n",
    "}\n",
    "\n",
    "# Print r2 scores\n",
    "# print('R-squared score (training): {:.3f}'\n",
    "#      .format(lassr_scores['r2_train']))\n",
    "# print('R-squared score (test): {:.3f}\\n'\n",
    "#      .format(lassr_scores['r2_test']))\n",
    "# print('R-squared score (test): {:.3f}\\n'\n",
    "#      .format(lassr_scores['r2_cv'].mean()))\n",
    "\n",
    "print(lassr_alpha)\n",
    "print(lassr_cv)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 0.253\n",
      "R-squared score (test): 0.026\n",
      "\n",
      "R-squared score (test): 0.862\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\talha\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\talha\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\talha\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(OH_X_data, y_data, random_state=0)\n",
    "\n",
    "knn_neighbors = 1\n",
    "# Define model\n",
    "knn = KNeighborsClassifier(n_neighbors = 5).fit(X_train, y_train.values.ravel())\n",
    "\n",
    "\n",
    "kf5 = KFold(n_splits=N, shuffle=True)\n",
    "\n",
    "# Get and save r2 scores for the model\n",
    "knn_scores = {\n",
    "     'optimal_nneighbors': knn_neighbors,\n",
    "     'r2_train': knn.score(X_train, y_train),\n",
    "     'r2_test': knn.score(X_test, y_test),\n",
    "     'r2_cv': cross_val_score(lassr, X_train, y_train, cv=kf5, scoring=\"r2\")\n",
    "}\n",
    "\n",
    "# Print r2 scores\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(knn_scores['r2_train']))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(knn_scores['r2_test']))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(knn_scores['r2_cv'].mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\talha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.6699856582194033\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(OH_X_data_scaled, y_data.values.ravel(), random_state=735)\n",
    "\n",
    "# Define and fit the model\n",
    "linsvc = LinearSVC(C=10).fit(X_train, y_train)\n",
    "\n",
    "kf5 = KFold(n_splits=N, shuffle=True)\n",
    "\n",
    "\n",
    "linsvc_c = 1\n",
    "linsvc_cv = 0\n",
    "kf5 = KFold(n_splits=N, shuffle=True) \n",
    "for this_c in [2, 3, 4]:\n",
    "    linsvc = LinearSVC(C=this_c, max_iter=9999).fit(X_train, y_train)\n",
    "    this_cv = cross_val_score(linsvc, X_train, y_train, cv=kf5, scoring='r2').mean()\n",
    "    if (this_cv > linsvc_cv):\n",
    "        linsvc_cv = this_cv\n",
    "        linsvc_c = this_c\n",
    "\n",
    "# Get and save r2 scores for the model\n",
    "linsvc_scores = {\n",
    "    'optimal_c': linsvc_c,\n",
    "    'r2_train': linsvc.score(X_train, y_train),\n",
    "    'r2_test': linsvc.score(X_test, y_test),\n",
    "    'r2_cv': cross_val_score(linsvc, X_train, y_train, cv=kf5, scoring=\"r2\")\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Print r2 scores\n",
    "# print('R-squared score (training): {:.3f}'\n",
    "#      .format(knn_scores['r2_train']))\n",
    "# print('R-squared score (test): {:.3f}\\n'\n",
    "#      .format(knn_scores['r2_test']))\n",
    "# print('R-squared score (test): {:.3f}\\n'\n",
    "#      .format(knn_scores['r2_cv'].mean()))\n",
    "\n",
    "print(linsvc_c)\n",
    "print(linsvc_cv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [156], line 13\u001b[0m\n\u001b[0;32m      7\u001b[0m svc \u001b[39m=\u001b[39m SVC(C\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, kernel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrbf\u001b[39m\u001b[39m\"\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      9\u001b[0m kf5 \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39mN, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m svc_scores \u001b[39m=\u001b[39m {\n\u001b[0;32m     12\u001b[0m     \u001b[39m'\u001b[39m\u001b[39moptimal_c\u001b[39m\u001b[39m'\u001b[39m: svc_c,\n\u001b[1;32m---> 13\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mr2_train\u001b[39m\u001b[39m'\u001b[39m: svc\u001b[39m.\u001b[39;49mscore(X_train, y_train),\n\u001b[0;32m     14\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mr2_test\u001b[39m\u001b[39m'\u001b[39m: svc\u001b[39m.\u001b[39mscore(X_test, y_test),\n\u001b[0;32m     15\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mr2_cv\u001b[39m\u001b[39m'\u001b[39m: cross_val_score(svc, X_train, y_train, cv\u001b[39m=\u001b[39mkf5, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m }\n\u001b[0;32m     17\u001b[0m svc_c \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     18\u001b[0m svc_cv \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\talha\\anaconda3\\lib\\site-packages\\sklearn\\base.py:651\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    649\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m--> 651\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\talha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:791\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    789\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_function(X), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 791\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m    792\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39masarray(y, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp))\n",
      "File \u001b[1;32mc:\\Users\\talha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:416\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    414\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_for_predict(X)\n\u001b[0;32m    415\u001b[0m predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse_predict \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dense_predict\n\u001b[1;32m--> 416\u001b[0m \u001b[39mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32mc:\\Users\\talha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:435\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    428\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mX.shape[1] = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m should be equal to \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe number of samples at training time\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m             \u001b[39m%\u001b[39m (X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_fit_[\u001b[39m0\u001b[39m])\n\u001b[0;32m    431\u001b[0m         )\n\u001b[0;32m    433\u001b[0m svm_type \u001b[39m=\u001b[39m LIBSVM_IMPL\u001b[39m.\u001b[39mindex(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl)\n\u001b[1;32m--> 435\u001b[0m \u001b[39mreturn\u001b[39;00m libsvm\u001b[39m.\u001b[39;49mpredict(\n\u001b[0;32m    436\u001b[0m     X,\n\u001b[0;32m    437\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msupport_,\n\u001b[0;32m    438\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msupport_vectors_,\n\u001b[0;32m    439\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_n_support,\n\u001b[0;32m    440\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dual_coef_,\n\u001b[0;32m    441\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_intercept_,\n\u001b[0;32m    442\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_probA,\n\u001b[0;32m    443\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_probB,\n\u001b[0;32m    444\u001b[0m     svm_type\u001b[39m=\u001b[39;49msvm_type,\n\u001b[0;32m    445\u001b[0m     kernel\u001b[39m=\u001b[39;49mkernel,\n\u001b[0;32m    446\u001b[0m     degree\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegree,\n\u001b[0;32m    447\u001b[0m     coef0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef0,\n\u001b[0;32m    448\u001b[0m     gamma\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma,\n\u001b[0;32m    449\u001b[0m     cache_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_size,\n\u001b[0;32m    450\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(OH_X_data_scaled, y_data.values.ravel())\n",
    "\n",
    "svc_c = 0.1\n",
    "svc = SVC(C=0.1, random_state=1, kernel=\"rbf\", gamma=\"auto\").fit(X_train, y_train)\n",
    "\n",
    "kf5 = KFold(n_splits=N, shuffle=True)\n",
    "\n",
    "svc_scores = {\n",
    "    'optimal_c': svc_c,\n",
    "    'r2_train': svc.score(X_train, y_train),\n",
    "    'r2_test': svc.score(X_test, y_test),\n",
    "    'r2_cv': cross_val_score(svc, X_train, y_train, cv=kf5, scoring='r2')\n",
    "}\n",
    "svc_c = 1\n",
    "svc_cv = 0\n",
    "kf5 = KFold(n_splits=N, shuffle=True) \n",
    "for this_c in [10, 1000, 10000]:\n",
    "    svc = SVC(C=this_c).fit(X_train, y_train)\n",
    "    this_cv = cross_val_score(svc, X_train, y_train, cv=kf5, scoring='r2').mean()\n",
    "    if (this_cv > svc_cv):\n",
    "        svc_cv = this_cv\n",
    "        svc_c = this_c\n",
    "\n",
    "print(svc_c)\n",
    "print(svc_cv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create excel file to export scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R2 Train</th>\n",
       "      <th>R2 Test</th>\n",
       "      <th>R2 CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.862290</td>\n",
       "      <td>0.889609</td>\n",
       "      <td>0.864140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polynomial Regression</th>\n",
       "      <td>0.866812</td>\n",
       "      <td>0.874708</td>\n",
       "      <td>0.868949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge Regression</th>\n",
       "      <td>0.862064</td>\n",
       "      <td>0.889970</td>\n",
       "      <td>0.862808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso Regression</th>\n",
       "      <td>0.866812</td>\n",
       "      <td>0.874707</td>\n",
       "      <td>0.868598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.252874</td>\n",
       "      <td>0.026207</td>\n",
       "      <td>0.862297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM (Non Linear)</th>\n",
       "      <td>0.012874</td>\n",
       "      <td>0.011034</td>\n",
       "      <td>-0.287620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM (Linear)</th>\n",
       "      <td>0.084138</td>\n",
       "      <td>0.016552</td>\n",
       "      <td>0.661135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       R2 Train   R2 Test     R2 CV\n",
       "Linear Regression      0.862290  0.889609  0.864140\n",
       "Polynomial Regression  0.866812  0.874708  0.868949\n",
       "Ridge Regression       0.862064  0.889970  0.862808\n",
       "Lasso Regression       0.866812  0.874707  0.868598\n",
       "KNN                    0.252874  0.026207  0.862297\n",
       "SVM (Non Linear)       0.012874  0.011034 -0.287620\n",
       "SVM (Linear)           0.084138  0.016552  0.661135"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create excel file includeing scores etc\n",
    "table = pd.DataFrame(columns=[\"R2 Train\", \"R2 Test\", \"R2 CV\"])\n",
    "table.loc[\"Linear Regression\"] = [linr_scores[\"r2_train\"], linr_scores[\"r2_test\"], linr_scores[\"r2_cv\"].mean()]\n",
    "table.loc[\"Polynomial Regression\"] = [polyr_scores[\"r2_train\"], polyr_scores[\"r2_test\"], polyr_scores[\"r2_cv\"].mean()]\n",
    "table.loc[\"Ridge Regression\"] = [ridgr_scores[\"r2_train\"], ridgr_scores[\"r2_test\"], ridgr_scores[\"r2_cv\"].mean()]\n",
    "table.loc[\"Lasso Regression\"] = [lassr_scores[\"r2_train\"], lassr_scores[\"r2_test\"], lassr_scores[\"r2_cv\"].mean()]\n",
    "table.loc[\"KNN\"] = [knn_scores[\"r2_train\"], knn_scores[\"r2_test\"], knn_scores[\"r2_cv\"].mean()]\n",
    "table.loc[\"SVM (Non Linear)\"] = [svc_scores[\"r2_train\"], svc_scores[\"r2_test\"], svc_scores[\"r2_cv\"].mean()]\n",
    "table.loc[\"SVM (Linear)\"] = [linsvc_scores[\"r2_train\"], linsvc_scores[\"r2_test\"], linsvc_scores[\"r2_cv\"].mean()]\n",
    "\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\talha\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Export Results\n",
    "price_predictions = linr.predict(OH_X_data_without_prices)\n",
    "price_predictions_knn = knn.predict(OH_X_data_without_prices)\n",
    "# price_predictions_int = np.round_(price_predictions)\n",
    "np.savetxt('linear.txt', price_predictions)\n",
    "np.savetxt('knn.txt', price_predictions_knn)\n",
    "# price_predictions_int\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a5bab918500c3a27c83c494b562b57d38406af52f32585b0235d82cba8b1288"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
